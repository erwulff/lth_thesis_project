{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first Variational Autoencoder for Jet compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "I will only use $p_T, \\eta, \\phi \\text{ and } E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../../data/'\n",
    "\n",
    "folder15 = 'breynold/user.breynold.data15_13TeV.00284484.physics_Main.DAOD_NTUP_JTRIG_JETM1.r9264_p3083_p3601_j042_tree.root/'\n",
    "file15 = 'user.breynold.18753218._000001.tree.root'\n",
    "folder16 = 'breynold/user.breynold.data16_13TeV.00307656.physics_Main.DAOD_NTUP_JTRIG_JETM1.r9264_p3083_p3601_j042_tree.root/'\n",
    "file16 = 'user.breynold.18797259._000001.tree.root'\n",
    "\n",
    "# Load a ROOT file\n",
    "filePath = path_to_data + folder16 + file16\n",
    "ttree = uproot.open(filePath)['outTree']['nominal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "branchnames = ['nAntiKt4EMTopoJets_Calib2018',\n",
    "               'AntiKt4EMTopoJets_Calib2018_E',\n",
    "               'AntiKt4EMTopoJets_Calib2018_pt',\n",
    "               'AntiKt4EMTopoJets_Calib2018_phi',\n",
    "               'AntiKt4EMTopoJets_Calib2018_eta']\n",
    "\n",
    "jaggedE = ttree.array(branchnames[1])\n",
    "jaggedpT = ttree.array(branchnames[2])\n",
    "jaggedphi = ttree.array(branchnames[3])\n",
    "jaggedeta = ttree.array(branchnames[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  9,  9, ...,  5, 18, 11])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaggedE.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([212.2173  , 175.15614 ,  64.451935, ...,  24.369474,  13.941364,\n",
       "        60.927433], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaggedE.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leading(jaggedX):\n",
    "    return jaggedX[jaggedX.counts > 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "leading_E = get_leading(jaggedE)\n",
    "leading_pT = get_leading(jaggedpT)\n",
    "leading_phi = get_leading(jaggedphi)\n",
    "leading_eta = get_leading(jaggedeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1937902,) (1937902,) (1937902,) (1937902,)\n"
     ]
    }
   ],
   "source": [
    "print(leading_E.shape, leading_eta.shape, leading_phi.shape, leading_pT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'pT': leading_pT, 'eta': leading_eta, 'phi': leading_phi, 'E': leading_E})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT</th>\n",
       "      <th>eta</th>\n",
       "      <th>phi</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>161.850494</td>\n",
       "      <td>-0.764774</td>\n",
       "      <td>2.287350</td>\n",
       "      <td>212.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>164.702454</td>\n",
       "      <td>0.205651</td>\n",
       "      <td>-1.074816</td>\n",
       "      <td>169.021805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>405.421387</td>\n",
       "      <td>-0.064094</td>\n",
       "      <td>-2.324020</td>\n",
       "      <td>407.661316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>139.671234</td>\n",
       "      <td>-0.289339</td>\n",
       "      <td>-2.052494</td>\n",
       "      <td>145.860703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>227.195618</td>\n",
       "      <td>2.158644</td>\n",
       "      <td>-1.864455</td>\n",
       "      <td>996.913025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>180.480667</td>\n",
       "      <td>1.684722</td>\n",
       "      <td>-1.049987</td>\n",
       "      <td>503.476410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>39.454185</td>\n",
       "      <td>-0.305395</td>\n",
       "      <td>0.755678</td>\n",
       "      <td>41.943340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>202.920883</td>\n",
       "      <td>1.583752</td>\n",
       "      <td>-0.301926</td>\n",
       "      <td>515.302856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>263.867432</td>\n",
       "      <td>-0.141594</td>\n",
       "      <td>-2.308791</td>\n",
       "      <td>267.712372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>234.460327</td>\n",
       "      <td>-2.949176</td>\n",
       "      <td>2.114750</td>\n",
       "      <td>2244.307617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pT       eta       phi            E\n",
       "0  161.850494 -0.764774  2.287350   212.217300\n",
       "1  164.702454  0.205651 -1.074816   169.021805\n",
       "2  405.421387 -0.064094 -2.324020   407.661316\n",
       "3  139.671234 -0.289339 -2.052494   145.860703\n",
       "4  227.195618  2.158644 -1.864455   996.913025\n",
       "5  180.480667  1.684722 -1.049987   503.476410\n",
       "6   39.454185 -0.305395  0.755678    41.943340\n",
       "7  202.920883  1.583752 -0.301926   515.302856\n",
       "8  263.867432 -0.141594 -2.308791   267.712372\n",
       "9  234.460327 -2.949176  2.114750  2244.307617"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(df.loc[0])\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1550321, 4) (387581, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "test = (test - train_mean) / train_std  # Is this the right way to normalize? (only using train mean and std to normalize both train and test)\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are building and AE\n",
    "test_y = test_x\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_features\n",
    "representation_size = input_size - 1\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.en1 = nn.Linear(input_size, 200)\n",
    "        self.en_mu = nn.Linear(200, representation_size)\n",
    "        self.en_std = nn.Linear(200, representation_size)\n",
    "        self.de1 = nn.Linear(representation_size, 200)\n",
    "        self.de2 = nn.Linear(200, input_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode a batch of samples, and return posterior parameters for each point.\"\"\"\n",
    "        h1 = self.tanh(self.en1(x))\n",
    "        return self.en_mu(h1), self.en_std(h1)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode a batch of latent variables\"\"\"\n",
    "        \n",
    "        h2 = self.tanh(self.de1(z))\n",
    "        return self.sigmoid(self.de2(h2))\n",
    "    \n",
    "    def reparam(self, mu, logvar):\n",
    "        \"\"\"Reparameterisation trick to sample z values. \n",
    "        This is stochastic during training,  and returns the mode during evaluation.\"\"\"\n",
    "        \n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Takes a batch of samples, encodes them, and then decodes them again to compare.\"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, input_size))\n",
    "        z = self.reparam(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def loss(self, reconstruction, x, mu, logvar):\n",
    "        \"\"\"ELBO assuming entries of x are binary variables, with closed form KLD.\"\"\"\n",
    "        \n",
    "        bce = torch.nn.functional.binary_cross_entropy(reconstruction, x.view(-1, input_size))\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        KLD /= x.view(-1, input_size).data.shape[0] * input_size\n",
    "\n",
    "        return bce + KLD\n",
    "    \n",
    "    def get_z(self, x):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "        \n",
    "        mu, logvar = self.encode(x.view(-1, input_size))\n",
    "        return self.reparam(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "\n",
    "def loss_batch(model, xb, yb, opt=None):\n",
    "    #loss = loss_func(model(xb), yb)\n",
    "    data = Variable(xb, requires_grad=False)\n",
    "    reco_b, mu, logvar = model(data)\n",
    "    loss = model.loss(reco_b, xb, mu, logvar)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "\n",
    "def fit(epochs, model, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)  # MSE-Loss\n",
    "        if(epoch % 1 == 0):\n",
    "            print('epoch: ' + str(epoch), 'validation loss: ' + str(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64  # batch size\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 validation loss: -24.151292958173794\n",
      "epoch: 1 validation loss: -25.149526606664597\n",
      "epoch: 2 validation loss: -24.622934093330315\n",
      "epoch: 3 validation loss: -23.92693139149605\n",
      "epoch: 4 validation loss: -23.53542617137865\n",
      "epoch: 5 validation loss: -23.12029424828167\n",
      "epoch: 6 validation loss: -23.107802431856054\n",
      "epoch: 7 validation loss: -23.30426460938009\n",
      "epoch: 8 validation loss: -23.612337796182757\n",
      "epoch: 9 validation loss: -23.769608544127063\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "fit(epochs, model, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: tensor([ 1.6863, -0.7095,  1.6946,  0.2721])\n",
      "Out: (tensor([[1.0000e+00, 1.3746e-29, 1.0000e+00, 4.8075e-01]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[-1.1429, -1.7562,  1.6886]], grad_fn=<AddmmBackward>), tensor([[-3.8708, -2.5482, -1.8221]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([ 0.1766, -0.1649, -0.0310, -0.5931])\n",
      "Out: (tensor([[3.2607e-01, 4.3352e-19, 4.7836e-30, 3.6263e-33]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[-0.3848, -0.6228, -0.6564]], grad_fn=<AddmmBackward>), tensor([[-3.1517, -3.5142, -1.4858]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([-0.3746,  0.6508,  1.3037, -0.5510])\n",
      "Out: (tensor([[0.0000e+00, 5.1504e-01, 1.0000e+00, 7.8305e-33]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[0.4120, 0.8205, 1.3976]], grad_fn=<AddmmBackward>), tensor([[-3.6306, -1.6341, -2.3438]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([-0.3711, -0.0072,  1.0894, -0.7293])\n",
      "Out: (tensor([[0.0000e+00, 1.0737e-18, 1.0000e+00, 1.1845e-32]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[-0.2140,  0.7788,  1.0828]], grad_fn=<AddmmBackward>), tensor([[-3.0836, -1.7094, -2.6808]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([-0.7818, -0.3965,  1.1572, -0.7662])\n",
      "Out: (tensor([[0.0000e+00, 8.3091e-20, 1.0000e+00, 2.1133e-32]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[-0.4946,  0.8662,  1.2114]], grad_fn=<AddmmBackward>), tensor([[-3.3045, -1.8727, -2.5291]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([0.4060, 1.5628, 1.4620, 1.8385])\n",
      "Out: (tensor([[0.5339, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward>), tensor([[ 2.1513, -0.6896,  1.5995]], grad_fn=<AddmmBackward>), tensor([[-2.8497, -3.4370, -2.1020]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([-0.0816, -0.7146, -1.2172, -0.3813])\n",
      "Out: (tensor([[0.0000e+00, 1.2202e-19, 4.2996e-30, 1.5711e-32]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[-0.4336,  0.6382, -0.9720]], grad_fn=<AddmmBackward>), tensor([[-3.4774, -1.1864, -2.0862]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([ 0.4208,  0.0398, -1.2887, -0.5597])\n",
      "Out: (tensor([[4.0301e-01, 2.5464e-01, 3.2496e-30, 6.7333e-33]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[ 0.0995, -0.7080, -0.9530]], grad_fn=<AddmmBackward>), tensor([[-3.3407, -3.6017, -2.0686]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([ 0.0139,  1.3978, -1.7089,  0.7964])\n",
      "Out: (tensor([[1.5569e-38, 1.0000e+00, 2.9006e-30, 8.5475e-01]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[ 1.5868,  0.1507, -0.9725]], grad_fn=<AddmmBackward>), tensor([[-3.6945, -1.6657, -2.2245]], grad_fn=<AddmmBackward>))\n",
      " \n",
      "Inp: tensor([ 1.8584,  0.4536, -0.0703, -0.0505])\n",
      "Out: (tensor([[1.0000e+00, 3.0931e-01, 4.4086e-30, 1.0818e-24]],\n",
      "       grad_fn=<SigmoidBackward>), tensor([[ 0.4870, -1.8138, -0.7576]], grad_fn=<AddmmBackward>), tensor([[-3.2579, -2.5501, -1.3854]], grad_fn=<AddmmBackward>))\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for ii in np.arange(100, 110):\n",
    "    data = valid_ds.tensors[0][ii]\n",
    "    pred = model(data)\n",
    "    print('Inp:', data)\n",
    "    print('Out:', pred)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (fastairoot)",
   "language": "python",
   "name": "fastairoot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
