{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "BIN = '../../'\n",
    "sys.path.append(BIN)\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import my_matplotlib_style as ms\n",
    "\n",
    "from fastai import basic_train, basic_data\n",
    "from fastai.callbacks import ActivationStats\n",
    "from fastai import train as tr\n",
    "\n",
    "from my_nn_modules import AE_big, AE_3D_50, AE_3D_50_bn_drop, AE_3D_50cone, AE_3D_100, AE_3D_100_bn_drop, AE_3D_100cone_bn_drop, AE_3D_200, AE_3D_200_bn_drop, AE_3D_500cone_bn\n",
    "from my_nn_modules import get_data, RMSELoss, plot_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../../../data/split_data/'\n",
    "\n",
    "save_dict = {}\n",
    "\n",
    "module = AE_big\n",
    "module_string = str(module).split(\"'\")[1].split(\".\")[1]\n",
    "model = module()\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "wd = 0\n",
    "pp = 0\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "bn_wd = False  # Don't use weight decay fpr batchnorm layers\n",
    "true_wd = True  # wd will be used for all optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "test = (test - train_mean) / train_std\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are training an AE\n",
    "test_y = test_x\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))\n",
    "\n",
    "bs = 1024\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs=bs)\n",
    "db = basic_data.DataBunch(train_dl, valid_dl)\n",
    "\n",
    "# loss_func = RMSELoss()\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "bn_wd = False  # Don't use weight decay fpr batchnorm layers\n",
    "true_wd = True  # wd will be used for all optimizers\n",
    "\n",
    "# Create Learner\n",
    "learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, bn_wd=bn_wd, true_wd=true_wd)\n",
    "# Train using the 1cycle policy\n",
    "start = time.perf_counter()\n",
    "learn.fit_one_cycle(epochs, max_lr=lr, wd=wd)\n",
    "end = time.perf_counter()\n",
    "delta_t = end - start\n",
    "time_string = str(datetime.timedelta(seconds=delta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_pickle(path_to_data + 'sub_train_%d' % 0)\n",
    "test = pd.read_pickle(path_to_data + 'sub_test_%d' % 0)\n",
    "\n",
    "# Normalize\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "test = (test - train_mean) / train_std\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are training an AE\n",
    "test_y = test_x\n",
    "\n",
    "# Create DataBunch\n",
    "bs = 1024\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs=bs)\n",
    "db = basic_data.DataBunch(train_dl, valid_dl)\n",
    "\n",
    "# Create Learner\n",
    "learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, bn_wd=bn_wd, true_wd=true_wd)\n",
    "# Train using the 1cycle policy\n",
    "start = time.perf_counter()\n",
    "learn.fit_one_cycle(epochs, max_lr=lr, wd=wd)\n",
    "end = time.perf_counter()\n",
    "delta_t = end - start\n",
    "time_string = str(datetime.timedelta(seconds=delta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (fastairoot)",
   "language": "python",
   "name": "fastairoot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
