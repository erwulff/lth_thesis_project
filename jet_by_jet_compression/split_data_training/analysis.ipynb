{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "BIN = '../../'\n",
    "sys.path.append(BIN)\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import my_matplotlib_style as ms\n",
    "\n",
    "from fastai import basic_train, basic_data\n",
    "from fastai.callbacks import ActivationStats\n",
    "from fastai import train as tr\n",
    "\n",
    "from my_nn_modules import AE_big, AE_3D_50, AE_3D_50_bn_drop, AE_3D_50cone, AE_3D_100, AE_3D_100_bn_drop, AE_3D_100cone_bn_drop, AE_3D_200, AE_3D_200_bn_drop, AE_3D_500cone_bn\n",
    "from my_nn_modules import get_data, RMSELoss, plot_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../../../data/split_data/'\n",
    "\n",
    "save_dict = {}\n",
    "\n",
    "module = AE_big\n",
    "module_string = str(module).split(\"'\")[1].split(\".\")[1]\n",
    "model = module()\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "wd = 0\n",
    "pp = 0\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "bn_wd = False  # Don't use weight decay fpr batchnorm layers\n",
    "true_wd = True  # wd will be used for all optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 0\n",
    "# Load data\n",
    "train = pd.read_pickle(path_to_data + 'sub_train_%d' % ii)\n",
    "test = pd.read_pickle(path_to_data + 'sub_test_%d' % ii)\n",
    "\n",
    "# Normalize\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "test = (test - train_mean) / train_std\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are training an AE\n",
    "test_y = test_x\n",
    "\n",
    "# Create DataBunch\n",
    "bs = 1024\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs=bs)\n",
    "db = basic_data.DataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE_big(\n",
       "  (en1): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (en2): Linear(in_features=8, out_features=6, bias=True)\n",
       "  (en3): Linear(in_features=6, out_features=4, bias=True)\n",
       "  (en4): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (de1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (de2): Linear(in_features=4, out_features=6, bias=True)\n",
       "  (de3): Linear(in_features=6, out_features=8, bias=True)\n",
       "  (de4): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, bn_wd=bn_wd, true_wd=true_wd)\n",
    "learn.load(module_string + '_subtrain_%d' % ii)\n",
    "learn.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0006899792]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate(dl=learn.data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00068135734]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate(dl=learn.data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def validate(learn, dl):\n",
    "    for batch in dl:\n",
    "        model = learn.model\n",
    "        losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb, yb in dl])\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(val_loss)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006899791656986553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006899791656986553"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(learn, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006813573956498598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006813573956498598"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(learn, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "for ii in np.arange(N):\n",
    "    \n",
    "    validate(learn, valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (fastairoot)",
   "language": "python",
   "name": "fastairoot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
