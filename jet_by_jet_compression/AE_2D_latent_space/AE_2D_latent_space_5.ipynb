{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Compression using an Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "BIN = '../../'\n",
    "sys.path.append(BIN)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from my_nn_modules import AE_big, get_data, fit\n",
    "\n",
    "import my_matplotlib_style as ms\n",
    "mpl.rc_file(BIN + 'my_matplotlib_rcparams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "force_cpu = False\n",
    "\n",
    "if force_cpu:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT</th>\n",
       "      <th>eta</th>\n",
       "      <th>phi</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1367643</td>\n",
       "      <td>300.752869</td>\n",
       "      <td>0.118391</td>\n",
       "      <td>-1.460827</td>\n",
       "      <td>303.719818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1739612</td>\n",
       "      <td>79.819145</td>\n",
       "      <td>4.535507</td>\n",
       "      <td>-1.185196</td>\n",
       "      <td>3722.832764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546066</td>\n",
       "      <td>220.472305</td>\n",
       "      <td>1.314261</td>\n",
       "      <td>-0.943992</td>\n",
       "      <td>440.195190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213212</td>\n",
       "      <td>74.533775</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>-0.424795</td>\n",
       "      <td>74.770332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150544</td>\n",
       "      <td>220.668121</td>\n",
       "      <td>2.432910</td>\n",
       "      <td>-2.308348</td>\n",
       "      <td>1266.681030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574837</td>\n",
       "      <td>159.841782</td>\n",
       "      <td>0.731125</td>\n",
       "      <td>-1.150598</td>\n",
       "      <td>207.133514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171687</td>\n",
       "      <td>306.125305</td>\n",
       "      <td>0.702473</td>\n",
       "      <td>0.437615</td>\n",
       "      <td>385.713013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608254</td>\n",
       "      <td>189.011673</td>\n",
       "      <td>-1.387443</td>\n",
       "      <td>0.490634</td>\n",
       "      <td>403.126709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1618830</td>\n",
       "      <td>194.996719</td>\n",
       "      <td>-1.022815</td>\n",
       "      <td>-1.560437</td>\n",
       "      <td>306.534515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637947</td>\n",
       "      <td>186.887146</td>\n",
       "      <td>-0.621282</td>\n",
       "      <td>-0.465523</td>\n",
       "      <td>226.002701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pT       eta       phi            E\n",
       "1367643  300.752869  0.118391 -1.460827   303.719818\n",
       "1739612   79.819145  4.535507 -1.185196  3722.832764\n",
       "546066   220.472305  1.314261 -0.943992   440.195190\n",
       "213212    74.533775  0.010658 -0.424795    74.770332\n",
       "150544   220.668121  2.432910 -2.308348  1266.681030\n",
       "574837   159.841782  0.731125 -1.150598   207.133514\n",
       "1171687  306.125305  0.702473  0.437615   385.713013\n",
       "1608254  189.011673 -1.387443  0.490634   403.126709\n",
       "1618830  194.996719 -1.022815 -1.560437   306.534515\n",
       "637947   186.887146 -0.621282 -0.465523   226.002701"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_pickle(BIN + 'processed_data/train.pkl')\n",
    "test = pd.read_pickle(BIN + 'processed_data/test.pkl')\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and splitting into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "# Is this the right way to normalize? (only using train mean and std to normalize both train and test)\n",
    "test = (test - train_mean) / train_std\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are building and AE\n",
    "test_y = test_x\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target, size_average=None, reduce=None, reduction='mean'):\n",
    "    # type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -> Tensor\n",
    "    r\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
    "\n",
    "    Measures the element-wise mean squared error.\n",
    "\n",
    "    See :class:`~torch.nn.MSELoss` for details.\n",
    "    \"\"\"\n",
    "    if not (target.size() == input.size()):\n",
    "        warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n",
    "                      \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "                      \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
    "                      stacklevel=2)\n",
    "    if size_average is not None or reduce is not None:\n",
    "        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "\n",
    "    ret = (input - target) ** 2\n",
    "    factor = torch.full(ret.size(), 1)\n",
    "    factor[:, 2] = 2.4\n",
    "    ret = ret * factor\n",
    "    if reduction != 'none':\n",
    "        ret = torch.mean(ret) if reduction == 'mean' else torch.sum(ret)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256  # batch size\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "loss_func = nn.MSELoss()\n",
    "loss_func = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_2D_v5(nn.Module):\n",
    "    def __init__(self, n_features=4):\n",
    "        super(AE_2D_v5, self).__init__()\n",
    "        self.en1 = nn.Linear(n_features, 200)\n",
    "        self.en2 = nn.Linear(200, 100)\n",
    "        self.en3 = nn.Linear(100, 50)\n",
    "        self.en4 = nn.Linear(50, 2)\n",
    "        self.de1 = nn.Linear(2, 50)\n",
    "        self.de2 = nn.Linear(50, 100)\n",
    "        self.de3 = nn.Linear(100, 200)\n",
    "        self.de4 = nn.Linear(200, n_features)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "\n",
    "class AE_big_2D_v1(nn.Module):\n",
    "    def __init__(self, n_features=4):\n",
    "        super(AE_big_2D_v1, self).__init__()\n",
    "        self.en1 = nn.Linear(n_features, 8)\n",
    "        self.en2 = nn.Linear(8, 6)\n",
    "        self.en3 = nn.Linear(6, 4)\n",
    "        self.en4 = nn.Linear(4, 2)\n",
    "        self.de1 = nn.Linear(2, 4)\n",
    "        self.de2 = nn.Linear(4, 6)\n",
    "        self.de3 = nn.Linear(6, 8)\n",
    "        self.de4 = nn.Linear(8, n_features)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "    \n",
    "\n",
    "class AE_big_2D_v2(nn.Module):\n",
    "    def __init__(self, n_features=4):\n",
    "        super(AE_big_2D_v2, self).__init__()\n",
    "        self.en1 = nn.Linear(n_features, 8)\n",
    "        self.en2 = nn.Linear(8, 6)\n",
    "        self.en3 = nn.Linear(6, 4)\n",
    "        self.en4 = nn.Linear(4, 3)\n",
    "        self.en5 = nn.Linear(3, 2)\n",
    "        self.de1 = nn.Linear(2, 3)\n",
    "        self.de2 = nn.Linear(3, 4)\n",
    "        self.de3 = nn.Linear(4, 6)\n",
    "        self.de4 = nn.Linear(6, 8)\n",
    "        self.de5 = nn.Linear(8, n_features)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.en5(self.tanh(self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.de5(self.tanh(self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(train.loc[0])\n",
    "model = AE_2D_v5(n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 1.0e-03\n",
      "Epoch 0: Validation loss = 0.08988853720801065 Time: 0:01:14.424069\n",
      "Epoch 1: Validation loss = 0.07854332009658106 Time: 0:02:32.593152\n",
      "Epoch 2: Validation loss = 0.06234419187615386 Time: 0:03:46.954169\n",
      "Epoch 3: Validation loss = 0.066418895839242 Time: 0:05:05.010953\n",
      "Epoch 4: Validation loss = 0.052982294005594364 Time: 0:06:23.034324\n",
      "Epoch 5: Validation loss = 0.047383356519985056 Time: 0:07:35.159751\n",
      "Epoch 6: Validation loss = 0.04619101984593646 Time: 0:08:38.077353\n",
      "Epoch 7: Validation loss = 0.04733169215196899 Time: 0:09:44.501752\n",
      "Epoch 8: Validation loss = 0.04587934123869099 Time: 0:11:06.152479\n",
      "Epoch 9: Validation loss = 0.04298225864859147 Time: 0:12:33.127514\n",
      "Epoch 10: Validation loss = 0.048444174363594834 Time: 0:14:01.665951\n",
      "Epoch 11: Validation loss = 0.04168629716062576 Time: 0:15:24.617892\n",
      "Epoch 12: Validation loss = 0.0459824026262705 Time: 0:16:46.665996\n",
      "Epoch 13: Validation loss = 0.044728424808413 Time: 0:18:10.145898\n",
      "Epoch 14: Validation loss = 0.043896844316421935 Time: 0:19:32.681043\n",
      "Epoch 15: Validation loss = 0.043654193928273174 Time: 0:20:56.602732\n",
      "Epoch 16: Validation loss = 0.03700614628279469 Time: 0:22:19.646905\n",
      "Epoch 17: Validation loss = 0.04467709494288262 Time: 0:23:43.253589\n",
      "Epoch 18: Validation loss = 0.03643117430008463 Time: 0:25:06.617217\n",
      "Epoch 19: Validation loss = 0.0383079098288022 Time: 0:26:30.390616\n",
      "Epoch 20: Validation loss = 0.042209483221039144 Time: 0:27:54.867769\n",
      "Epoch 21: Validation loss = 0.05018411793621168 Time: 0:29:27.111469\n",
      "Epoch 22: Validation loss = 0.036708208248737974 Time: 0:30:54.268561\n",
      "Epoch 23: Validation loss = 0.04315441149289254 Time: 0:32:29.250580\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "#epochs_list = [7, 5, 3, 2, 2]\n",
    "epochs_list = [40, 40, 20, 20, 20]\n",
    "lrs = [1e-3, 3e-4, 1e-4, 3e-5, 1e-5]\n",
    "for ii, epochs in enumerate(epochs_list):\n",
    "    print('Setting learning rate to %.1e' % lrs[ii])\n",
    "    opt = optim.Adam(model.parameters(), lr=lrs[ii])\n",
    "    #opt = optim.SGD(model.parameters(), lr=lrs[ii], momentum=0.9, nesterov=True)\n",
    "    #opt = optim.RMSprop(model.parameters(), lr=lrs[ii], momentum=0.9)\n",
    "    fit(epochs, model, loss_func, opt, train_dl, valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "#epochs_list = [7, 5, 3, 2, 2]\n",
    "epochs_list = [40, 40, 20, 20, 20]\n",
    "lrs = [1e-4, 3e-5, 1e-5, 3e-6, 1e-6]\n",
    "for ii, epochs in enumerate(epochs_list):\n",
    "    print('Setting learning rate to %.1e' % lrs[ii])\n",
    "    opt = optim.Adam(model.parameters(), lr=lrs[ii])\n",
    "    #opt = optim.SGD(model.parameters(), lr=lrs[ii], momentum=0.9, nesterov=True)\n",
    "    #opt = optim.RMSprop(model.parameters(), lr=lrs[ii], momentum=0.9)\n",
    "    fit(epochs, model, loss_func, opt, train_dl, valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model for later inference (if training is to be continued another saving method is recommended)\n",
    "#save_path = './models/AE_2D_v3_bs256_loss00346.pt'\n",
    "#torch.save(model.state_dict(), save_path)\n",
    "# model_big = AE_big()\n",
    "# model_big.load_state_dict(torch.load(save_path))\n",
    "# model_big.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few tensors, still normalized\n",
    "print('Comparing input and output:')\n",
    "for ii in np.arange(100, 105):\n",
    "    data = valid_ds.tensors[0][ii]\n",
    "    pred = model(data)\n",
    "    print('Inp:', data)\n",
    "    print('Out:', pred)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few tensors, now not normalized\n",
    "print('Comparing input and output:')\n",
    "for ii in np.arange(100, 105):\n",
    "    data = valid_ds.tensors[0][ii]\n",
    "    pred = model(data).detach().numpy()\n",
    "    pred = np.multiply(pred, train_std.values)\n",
    "    pred = np.add(pred, train_mean.values)\n",
    "    data = np.multiply(data, train_std.values)\n",
    "    data = np.add(data, train_mean.values)\n",
    "    print('Inp:', data)\n",
    "    print('Out:', pred)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']\n",
    "variable_list = [r'$p_T$', r'$\\eta$', r'$\\phi$', r'$E$']\n",
    "line_style = ['--', '-']\n",
    "colors = ['orange', 'c']\n",
    "markers = ['*', 's']\n",
    "\n",
    "\n",
    "# Histograms\n",
    "idxs = (0, 100000)  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "pred = model(data).detach().numpy()\n",
    "pred = np.multiply(pred, train_std.values)\n",
    "pred = np.add(pred, train_mean.values)\n",
    "data = np.multiply(data, train_std.values)\n",
    "data = np.add(data, train_mean.values)\n",
    "\n",
    "alph = 0.8\n",
    "n_bins = 50\n",
    "for kk in np.arange(4):\n",
    "    plt.figure(kk + 4)\n",
    "    n_hist_data, bin_edges, _ = plt.hist(data[:, kk], color=colors[1], label='Input', alpha=1, bins=n_bins)\n",
    "    n_hist_pred, _, _ = plt.hist(pred[:, kk], color=colors[0], label='Output', alpha=alph, bins=bin_edges)\n",
    "    plt.suptitle(train.columns[kk])\n",
    "    plt.xlabel(variable_list[kk] + ' ' + unit_list[kk])\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot input on top of output\n",
    "idxs = (0, 100)  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "pred = model(data).detach().numpy()\n",
    "pred = np.multiply(pred, train_std.values)\n",
    "pred = np.add(pred, train_mean.values)\n",
    "data = np.multiply(data, train_std.values)\n",
    "data = np.add(data, train_mean.values)\n",
    "\n",
    "for kk in np.arange(4):\n",
    "    plt.figure(kk)\n",
    "    plt.plot(data[:, kk], color=colors[1], label='Input', linestyle=line_style[1], marker=markers[1])\n",
    "    plt.plot(pred[:, kk], color=colors[0], label='Output', linestyle=line_style[0], marker=markers[0])\n",
    "    plt.suptitle(train.columns[kk])\n",
    "    plt.xlabel('Event')\n",
    "    plt.ylabel(variable_list[kk] + ' ' + unit_list[kk])\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input on top of output\n",
    "idxs = (0, int(1e5))  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "latent = model.encode(data).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in np.arange(latent.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.hist(latent[:, ii], label='$z_%d$' % (ii + 1), color='m')\n",
    "    plt.suptitle('Latent variable #%d' % (ii + 1))\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mksz = 1\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent[:, 0], latent[:, 1], s=mksz)\n",
    "plt.xlabel(r'$z_1$')\n",
    "plt.ylabel(r'$z_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (fastairoot)",
   "language": "python",
   "name": "fastairoot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
