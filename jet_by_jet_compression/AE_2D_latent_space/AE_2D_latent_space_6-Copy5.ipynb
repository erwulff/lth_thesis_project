{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Compression using an Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "BIN = '../../'\n",
    "sys.path.append(BIN)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from my_nn_modules import AE_big, get_data, fit\n",
    "\n",
    "import my_matplotlib_style as ms\n",
    "mpl.rc_file(BIN + 'my_matplotlib_rcparams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "force_cpu = False\n",
    "\n",
    "if force_cpu:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT</th>\n",
       "      <th>eta</th>\n",
       "      <th>phi</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1367643</td>\n",
       "      <td>300.752869</td>\n",
       "      <td>0.118391</td>\n",
       "      <td>-1.460827</td>\n",
       "      <td>303.719818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1739612</td>\n",
       "      <td>79.819145</td>\n",
       "      <td>4.535507</td>\n",
       "      <td>-1.185196</td>\n",
       "      <td>3722.832764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546066</td>\n",
       "      <td>220.472305</td>\n",
       "      <td>1.314261</td>\n",
       "      <td>-0.943992</td>\n",
       "      <td>440.195190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213212</td>\n",
       "      <td>74.533775</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>-0.424795</td>\n",
       "      <td>74.770332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150544</td>\n",
       "      <td>220.668121</td>\n",
       "      <td>2.432910</td>\n",
       "      <td>-2.308348</td>\n",
       "      <td>1266.681030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574837</td>\n",
       "      <td>159.841782</td>\n",
       "      <td>0.731125</td>\n",
       "      <td>-1.150598</td>\n",
       "      <td>207.133514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171687</td>\n",
       "      <td>306.125305</td>\n",
       "      <td>0.702473</td>\n",
       "      <td>0.437615</td>\n",
       "      <td>385.713013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608254</td>\n",
       "      <td>189.011673</td>\n",
       "      <td>-1.387443</td>\n",
       "      <td>0.490634</td>\n",
       "      <td>403.126709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1618830</td>\n",
       "      <td>194.996719</td>\n",
       "      <td>-1.022815</td>\n",
       "      <td>-1.560437</td>\n",
       "      <td>306.534515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637947</td>\n",
       "      <td>186.887146</td>\n",
       "      <td>-0.621282</td>\n",
       "      <td>-0.465523</td>\n",
       "      <td>226.002701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pT       eta       phi            E\n",
       "1367643  300.752869  0.118391 -1.460827   303.719818\n",
       "1739612   79.819145  4.535507 -1.185196  3722.832764\n",
       "546066   220.472305  1.314261 -0.943992   440.195190\n",
       "213212    74.533775  0.010658 -0.424795    74.770332\n",
       "150544   220.668121  2.432910 -2.308348  1266.681030\n",
       "574837   159.841782  0.731125 -1.150598   207.133514\n",
       "1171687  306.125305  0.702473  0.437615   385.713013\n",
       "1608254  189.011673 -1.387443  0.490634   403.126709\n",
       "1618830  194.996719 -1.022815 -1.560437   306.534515\n",
       "637947   186.887146 -0.621282 -0.465523   226.002701"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_pickle(BIN + 'processed_data/train.pkl')\n",
    "test = pd.read_pickle(BIN + 'processed_data/test.pkl')\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and splitting into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "# Is this the right way to normalize? (only using train mean and std to normalize both train and test)\n",
    "test = (test - train_mean) / train_std\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are building and AE\n",
    "test_y = test_x\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target, size_average=None, reduce=None, reduction='mean'):\n",
    "    # type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -> Tensor\n",
    "    r\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
    "\n",
    "    Measures the element-wise mean squared error.\n",
    "\n",
    "    See :class:`~torch.nn.MSELoss` for details.\n",
    "    \"\"\"\n",
    "    if not (target.size() == input.size()):\n",
    "        warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n",
    "                      \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "                      \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
    "                      stacklevel=2)\n",
    "    if size_average is not None or reduce is not None:\n",
    "        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "\n",
    "    ret = (input - target) ** 2\n",
    "    factor = torch.full(ret.size(), 1)\n",
    "    factor[:, 2] = 2.4\n",
    "    ret = ret * factor\n",
    "    if reduction != 'none':\n",
    "        ret = torch.mean(ret) if reduction == 'mean' else torch.sum(ret)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256  # batch size\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "loss_func = nn.MSELoss()\n",
    "loss_func = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_2D_v200(nn.Module):\n",
    "    def __init__(self, n_features=4):\n",
    "        super(AE_2D_v200, self).__init__()\n",
    "        self.en1 = nn.Linear(n_features, 200)\n",
    "        self.en2 = nn.Linear(200, 200)\n",
    "        self.en3 = nn.Linear(200, 200)\n",
    "        self.en4 = nn.Linear(200, 2)\n",
    "        self.de1 = nn.Linear(2, 200)\n",
    "        self.de2 = nn.Linear(200, 200)\n",
    "        self.de3 = nn.Linear(200, 200)\n",
    "        self.de4 = nn.Linear(200, n_features)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(train.loc[0])\n",
    "model = AE_2D_v200(n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 1.0e-03\n",
      "Epoch 0: Validation loss = 0.13594044191633803 Time: 0:02:41.384107\n",
      "Epoch 1: Validation loss = 0.13818707880988618 Time: 0:04:52.679778\n",
      "Epoch 2: Validation loss = 0.06671269680361416 Time: 0:07:07.021613\n",
      "Epoch 3: Validation loss = 0.05388856501750008 Time: 0:09:25.072279\n",
      "Epoch 4: Validation loss = 0.047582941947826585 Time: 0:11:37.861837\n",
      "Epoch 5: Validation loss = 0.04575329816088675 Time: 0:14:01.920800\n",
      "Epoch 6: Validation loss = 0.039932629884080646 Time: 0:16:21.688385\n",
      "Epoch 7: Validation loss = 0.03770655901307644 Time: 0:18:44.217058\n",
      "Epoch 8: Validation loss = 0.041114305492305186 Time: 0:20:49.495613\n",
      "Epoch 9: Validation loss = 0.055588596093631516 Time: 0:23:05.947859\n",
      "Epoch 10: Validation loss = 0.03791319135102046 Time: 0:25:13.218617\n",
      "Epoch 11: Validation loss = 0.03662667624310461 Time: 0:27:37.983169\n",
      "Epoch 12: Validation loss = 0.03633540853273837 Time: 0:30:15.387924\n",
      "Epoch 13: Validation loss = 0.03302988892699484 Time: 0:32:46.769869\n",
      "Epoch 14: Validation loss = 0.040004523666993634 Time: 0:35:11.783501\n",
      "Epoch 15: Validation loss = 0.036175390325609326 Time: 0:37:27.228122\n",
      "Epoch 16: Validation loss = 0.03372277320794236 Time: 0:39:26.274855\n",
      "Epoch 17: Validation loss = 0.03849320988265576 Time: 0:41:41.607216\n",
      "Epoch 18: Validation loss = 0.03174138699360886 Time: 0:43:58.371540\n",
      "Epoch 19: Validation loss = 0.03553350784393193 Time: 0:46:09.439887\n",
      "Epoch 20: Validation loss = 0.030676302402849773 Time: 0:48:26.145892\n",
      "Epoch 21: Validation loss = 0.04137349398174029 Time: 0:50:35.050292\n",
      "Epoch 22: Validation loss = 0.15412434640515385 Time: 0:52:43.910525\n",
      "Epoch 23: Validation loss = 0.057637455425483416 Time: 0:55:11.450177\n",
      "Epoch 24: Validation loss = 0.04659082797038667 Time: 0:57:29.082305\n",
      "Epoch 25: Validation loss = 0.04560601723582825 Time: 0:59:37.804226\n",
      "Epoch 26: Validation loss = 0.05005402535783815 Time: 1:01:51.759239\n",
      "Epoch 27: Validation loss = 0.04760058830734337 Time: 1:04:15.736170\n",
      "Epoch 28: Validation loss = 0.05129722956262343 Time: 1:06:29.477277\n",
      "Epoch 29: Validation loss = 0.05067461213644537 Time: 1:08:46.066762\n",
      "Epoch 30: Validation loss = 0.05648792981257346 Time: 1:11:06.304501\n",
      "Epoch 31: Validation loss = 0.05210935347654118 Time: 1:13:26.178732\n",
      "Epoch 32: Validation loss = 0.05034562484971078 Time: 1:15:38.471321\n",
      "Epoch 33: Validation loss = 0.06024198645375978 Time: 1:17:45.346996\n",
      "Epoch 34: Validation loss = 0.05537406877145171 Time: 1:19:58.410864\n",
      "Epoch 35: Validation loss = 0.05066020095600506 Time: 1:22:06.373717\n",
      "Epoch 36: Validation loss = 0.07116853423696179 Time: 1:24:16.539684\n",
      "Epoch 37: Validation loss = 0.062312571900171074 Time: 1:26:32.581821\n",
      "Epoch 38: Validation loss = 0.04643491523288687 Time: 1:28:49.958768\n",
      "Epoch 39: Validation loss = 0.06476441643588637 Time: 1:31:10.353424\n",
      "Setting learning rate to 3.0e-04\n",
      "Epoch 0: Validation loss = 0.03475154104264431 Time: 0:02:16.451291\n",
      "Epoch 1: Validation loss = 0.03324958135920906 Time: 0:04:35.439509\n",
      "Epoch 2: Validation loss = 0.03297945633762354 Time: 0:06:45.189835\n",
      "Epoch 3: Validation loss = 0.03052374291683642 Time: 0:09:02.056804\n",
      "Epoch 4: Validation loss = 0.03135051917191773 Time: 0:10:58.567126\n",
      "Epoch 5: Validation loss = 0.029859265572494537 Time: 0:13:11.489453\n",
      "Epoch 6: Validation loss = 0.029959813686881855 Time: 0:15:23.929575\n",
      "Epoch 7: Validation loss = 0.031424179735545614 Time: 0:17:33.610605\n",
      "Epoch 8: Validation loss = 0.03155889354332501 Time: 0:19:54.775306\n",
      "Epoch 9: Validation loss = 0.036937687669644545 Time: 0:22:13.951777\n",
      "Epoch 10: Validation loss = 0.03916916784042446 Time: 0:24:25.306869\n",
      "Epoch 11: Validation loss = 0.03435651849564597 Time: 0:26:40.957488\n",
      "Epoch 12: Validation loss = 0.03757061214555961 Time: 0:28:53.071132\n",
      "Epoch 13: Validation loss = 0.1955803245224739 Time: 0:30:59.401725\n",
      "Epoch 14: Validation loss = 0.04089659705225533 Time: 0:33:04.050474\n",
      "Epoch 15: Validation loss = 0.040018142620417675 Time: 0:35:41.165178\n",
      "Epoch 16: Validation loss = 0.03758202728435733 Time: 0:38:29.737965\n",
      "Epoch 17: Validation loss = 0.035305732781504735 Time: 0:40:45.590084\n",
      "Epoch 18: Validation loss = 0.03930873417624876 Time: 0:43:04.804424\n",
      "Epoch 19: Validation loss = 0.06170163630862241 Time: 0:45:26.063966\n",
      "Epoch 20: Validation loss = 0.03600998700320891 Time: 0:47:47.309464\n",
      "Epoch 21: Validation loss = 0.03923629072347033 Time: 0:50:02.851640\n",
      "Epoch 22: Validation loss = 0.03906609149857942 Time: 0:52:20.515752\n",
      "Epoch 23: Validation loss = 0.0511423770570197 Time: 0:54:35.100633\n",
      "Epoch 24: Validation loss = 0.03474896036346424 Time: 0:56:55.892904\n",
      "Epoch 25: Validation loss = 0.03799113423982999 Time: 0:59:15.361732\n",
      "Epoch 26: Validation loss = 0.04107487767134995 Time: 1:01:34.460546\n",
      "Epoch 27: Validation loss = 0.0382064931881321 Time: 1:03:47.584892\n",
      "Epoch 28: Validation loss = 0.03873314137631961 Time: 1:06:07.630091\n",
      "Epoch 29: Validation loss = 0.04415542028690782 Time: 1:08:23.137156\n",
      "Epoch 30: Validation loss = 0.04159232930962471 Time: 1:10:38.087644\n",
      "Epoch 31: Validation loss = 0.040618937315173735 Time: 1:12:53.589338\n",
      "Epoch 32: Validation loss = 0.03937807643785299 Time: 1:15:06.770780\n",
      "Epoch 33: Validation loss = 0.04065741147059761 Time: 1:17:29.166928\n",
      "Epoch 34: Validation loss = 0.04482502600911111 Time: 1:19:51.964105\n",
      "Epoch 35: Validation loss = 0.05716331175034901 Time: 1:22:10.513311\n",
      "Epoch 36: Validation loss = 0.04925339661006712 Time: 1:24:22.228212\n",
      "Epoch 37: Validation loss = 0.04470594015174456 Time: 1:27:20.713664\n",
      "Epoch 38: Validation loss = 0.04626033891552983 Time: 1:29:51.979775\n",
      "Epoch 39: Validation loss = 0.045210138450470734 Time: 1:32:02.218156\n",
      "Setting learning rate to 1.0e-04\n",
      "Epoch 0: Validation loss = 0.04243386629761991 Time: 0:02:03.042549\n",
      "Epoch 1: Validation loss = 0.04311836340779248 Time: 0:04:15.509386\n",
      "Epoch 2: Validation loss = 0.045091193144038325 Time: 0:06:28.174467\n",
      "Epoch 3: Validation loss = 0.0428287946029657 Time: 0:08:44.024729\n",
      "Epoch 4: Validation loss = 0.041140374184611875 Time: 0:10:55.405204\n",
      "Epoch 5: Validation loss = 0.04159146948549785 Time: 0:13:15.285029\n",
      "Epoch 6: Validation loss = 0.04234481244525529 Time: 0:15:41.179915\n",
      "Epoch 7: Validation loss = 0.04026891799127428 Time: 0:18:15.716074\n",
      "Epoch 8: Validation loss = 0.04156325283506284 Time: 0:20:38.257601\n",
      "Epoch 9: Validation loss = 0.041830641270814324 Time: 0:23:10.313563\n",
      "Epoch 10: Validation loss = 0.04195203969890607 Time: 0:25:34.861578\n",
      "Epoch 11: Validation loss = 0.04189474518445004 Time: 0:27:44.892045\n",
      "Epoch 12: Validation loss = 0.041017123819375834 Time: 0:30:18.610285\n",
      "Epoch 13: Validation loss = 0.04100366987300089 Time: 0:32:45.813128\n",
      "Epoch 14: Validation loss = 0.04335716775140653 Time: 0:34:58.302220\n",
      "Epoch 15: Validation loss = 0.0395996167926588 Time: 0:37:19.414963\n",
      "Epoch 16: Validation loss = 0.0397278389790332 Time: 0:39:18.889516\n",
      "Epoch 17: Validation loss = 0.0396052859360344 Time: 0:41:24.931948\n",
      "Epoch 18: Validation loss = 0.03933240256820055 Time: 0:43:58.994496\n",
      "Epoch 19: Validation loss = 0.04003931583593592 Time: 0:46:14.778618\n",
      "Setting learning rate to 3.0e-05\n",
      "Epoch 0: Validation loss = 0.037812195756091016 Time: 0:02:20.715139\n",
      "Epoch 1: Validation loss = 0.03830269536868908 Time: 0:04:26.179047\n",
      "Epoch 2: Validation loss = 0.0369232737858671 Time: 0:06:38.352732\n",
      "Epoch 3: Validation loss = 0.037203033107743096 Time: 0:08:55.373944\n",
      "Epoch 4: Validation loss = 0.03706663892548017 Time: 0:11:06.462421\n",
      "Epoch 5: Validation loss = 0.0368586408718491 Time: 0:13:52.009544\n",
      "Epoch 6: Validation loss = 0.03664715953662043 Time: 0:16:19.124376\n",
      "Epoch 7: Validation loss = 0.03728181397290499 Time: 0:18:42.555141\n",
      "Epoch 8: Validation loss = 0.03695320831723416 Time: 0:20:32.967922\n",
      "Epoch 9: Validation loss = 0.03722058646251048 Time: 0:22:21.986305\n",
      "Epoch 10: Validation loss = 0.03721066768220515 Time: 0:23:57.280106\n",
      "Epoch 11: Validation loss = 0.037203100817944285 Time: 0:25:26.570701\n",
      "Epoch 12: Validation loss = 0.036443128822203796 Time: 0:27:04.189851\n",
      "Epoch 13: Validation loss = 0.036083857170151605 Time: 0:28:38.477948\n",
      "Epoch 14: Validation loss = 0.03618965418712496 Time: 0:30:48.263597\n",
      "Epoch 15: Validation loss = 0.037817416483331937 Time: 0:32:52.092839\n",
      "Epoch 16: Validation loss = 0.0361078380319827 Time: 0:34:54.905386\n",
      "Epoch 17: Validation loss = 0.03611601679302631 Time: 0:36:57.387183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Validation loss = 0.0359162006372659 Time: 0:38:54.441883\n",
      "Epoch 19: Validation loss = 0.03608291271686125 Time: 0:40:56.362190\n",
      "Setting learning rate to 1.0e-05\n",
      "Epoch 0: Validation loss = 0.03583850114885109 Time: 0:01:53.491911\n",
      "Epoch 1: Validation loss = 0.03587050324429887 Time: 0:03:52.896010\n",
      "Epoch 2: Validation loss = 0.03633466362863317 Time: 0:05:53.436918\n",
      "Epoch 3: Validation loss = 0.03576604600214862 Time: 0:07:44.995552\n",
      "Epoch 4: Validation loss = 0.03593632968142009 Time: 0:09:43.387661\n",
      "Epoch 5: Validation loss = 0.035751590640333994 Time: 0:11:50.642471\n",
      "Epoch 6: Validation loss = 0.03564822089626921 Time: 0:13:57.604572\n",
      "Epoch 7: Validation loss = 0.03618637544804768 Time: 0:15:58.290585\n",
      "Epoch 8: Validation loss = 0.03589097626761049 Time: 0:18:01.920493\n",
      "Epoch 9: Validation loss = 0.03586580859808621 Time: 0:20:04.404960\n",
      "Epoch 10: Validation loss = 0.03591295706982002 Time: 0:22:06.968023\n",
      "Epoch 11: Validation loss = 0.03584972170306964 Time: 0:24:08.169117\n",
      "Epoch 12: Validation loss = 0.03584546935129282 Time: 0:26:07.191961\n",
      "Epoch 13: Validation loss = 0.03584788512475454 Time: 0:28:07.923967\n",
      "Epoch 14: Validation loss = 0.036201037042383435 Time: 0:30:16.098241\n",
      "Epoch 15: Validation loss = 0.03591062012747101 Time: 0:32:22.773405\n",
      "Epoch 16: Validation loss = 0.03575873257248238 Time: 0:34:24.116805\n",
      "Epoch 17: Validation loss = 0.03592957050140869 Time: 0:36:30.226265\n",
      "Epoch 18: Validation loss = 0.03556435882241394 Time: 0:38:35.056897\n",
      "Epoch 19: Validation loss = 0.03566514924271816 Time: 0:40:38.016108\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "#epochs_list = [7, 5, 3, 2, 2]\n",
    "epochs_list = [40, 40, 20, 20, 20]\n",
    "lrs = [1e-3, 3e-4, 1e-4, 3e-5, 1e-5]\n",
    "for ii, epochs in enumerate(epochs_list):\n",
    "    print('Setting learning rate to %.1e' % lrs[ii])\n",
    "    opt = optim.Adam(model.parameters(), lr=lrs[ii])\n",
    "    #opt = optim.SGD(model.parameters(), lr=lrs[ii], momentum=0.9, nesterov=True)\n",
    "    #opt = optim.RMSprop(model.parameters(), lr=lrs[ii], momentum=0.9)\n",
    "    fit(epochs, model, loss_func, opt, train_dl, valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 1.0e-04\n",
      "Epoch 0: Validation loss = 0.040168988096631546 Time: 0:02:02.738849\n",
      "Epoch 1: Validation loss = 0.03904334575117154 Time: 0:04:04.531252\n",
      "Epoch 2: Validation loss = 0.03799365354283852 Time: 0:06:06.952934\n",
      "Epoch 3: Validation loss = 0.0394356475409341 Time: 0:08:10.684928\n",
      "Epoch 4: Validation loss = 0.039405331949631923 Time: 0:10:18.420197\n",
      "Epoch 5: Validation loss = 0.039100810284797796 Time: 0:12:15.440312\n",
      "Epoch 6: Validation loss = 0.03847510450622685 Time: 0:14:10.671532\n",
      "Epoch 7: Validation loss = 0.039538633310083046 Time: 0:16:15.618707\n",
      "Epoch 8: Validation loss = 0.038955094921375784 Time: 0:18:10.332438\n",
      "Epoch 9: Validation loss = 0.0387374168931995 Time: 0:20:03.436132\n",
      "Epoch 10: Validation loss = 0.03812910713189027 Time: 0:21:57.489218\n",
      "Epoch 11: Validation loss = 0.03810490061685725 Time: 0:23:48.617147\n",
      "Epoch 12: Validation loss = 0.03875082779374368 Time: 0:25:40.478821\n",
      "Epoch 13: Validation loss = 0.03862246022788678 Time: 0:27:32.332625\n",
      "Epoch 14: Validation loss = 0.0391991761516589 Time: 0:29:23.407970\n",
      "Epoch 15: Validation loss = 0.038613966185159346 Time: 0:31:15.885822\n",
      "Epoch 16: Validation loss = 0.04400875487221103 Time: 0:33:09.385808\n",
      "Epoch 17: Validation loss = 0.038786111887336674 Time: 0:35:02.543514\n",
      "Epoch 18: Validation loss = 0.037700880669041856 Time: 0:36:58.479636\n",
      "Epoch 19: Validation loss = 0.03880138346064881 Time: 0:38:57.772409\n",
      "Epoch 20: Validation loss = 0.03863592350275075 Time: 0:40:51.210279\n",
      "Epoch 21: Validation loss = 0.038657660912202026 Time: 0:42:49.056579\n",
      "Epoch 22: Validation loss = 0.03941621422469063 Time: 0:44:46.164386\n",
      "Epoch 23: Validation loss = 0.03926235935181153 Time: 0:46:45.807399\n",
      "Epoch 24: Validation loss = 0.03705351122309405 Time: 0:48:42.854564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-32560f142c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#opt = optim.SGD(model.parameters(), lr=lrs[ii], momentum=0.9, nesterov=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#opt = optim.RMSprop(model.parameters(), lr=lrs[ii], momentum=0.9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/LTH/Examensarbete/lth_thesis_project/my_nn_modules.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, train_dl, valid_dl, device)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/LTH/Examensarbete/lth_thesis_project/my_nn_modules.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, loss_func, xb, yb, opt)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE_2D_v3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastairoot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-44a1dec07f10>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-44a1dec07f10>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mde4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mde3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mde2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mde1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastairoot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastairoot/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastairoot/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "#epochs_list = [7, 5, 3, 2, 2]\n",
    "epochs_list = [40, 40, 20, 20, 20]\n",
    "lrs = [1e-4, 3e-5, 1e-5, 3e-6, 1e-6]\n",
    "for ii, epochs in enumerate(epochs_list):\n",
    "    print('Setting learning rate to %.1e' % lrs[ii])\n",
    "    opt = optim.Adam(model.parameters(), lr=lrs[ii])\n",
    "    #opt = optim.SGD(model.parameters(), lr=lrs[ii], momentum=0.9, nesterov=True)\n",
    "    #opt = optim.RMSprop(model.parameters(), lr=lrs[ii], momentum=0.9)\n",
    "    fit(epochs, model, loss_func, opt, train_dl, valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model for later inference (if training is to be continued another saving method is recommended)\n",
    "#save_path = './models/AE_2D_v4_bs256_loss0029.pt'\n",
    "#torch.save(model.state_dict(), save_path)\n",
    "# model_big = AE_big()\n",
    "# model_big.load_state_dict(torch.load(save_path))\n",
    "# model_big.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few tensors, still normalized\n",
    "print('Comparing input and output:')\n",
    "for ii in np.arange(100, 105):\n",
    "    data = valid_ds.tensors[0][ii]\n",
    "    pred = model(data)\n",
    "    print('Inp:', data)\n",
    "    print('Out:', pred)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few tensors, now not normalized\n",
    "print('Comparing input and output:')\n",
    "for ii in np.arange(100, 105):\n",
    "    data = valid_ds.tensors[0][ii]\n",
    "    pred = model(data).detach().numpy()\n",
    "    pred = np.multiply(pred, train_std.values)\n",
    "    pred = np.add(pred, train_mean.values)\n",
    "    data = np.multiply(data, train_std.values)\n",
    "    data = np.add(data, train_mean.values)\n",
    "    print('Inp:', data)\n",
    "    print('Out:', pred)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']\n",
    "variable_list = [r'$p_T$', r'$\\eta$', r'$\\phi$', r'$E$']\n",
    "line_style = ['--', '-']\n",
    "colors = ['orange', 'c']\n",
    "markers = ['*', 's']\n",
    "\n",
    "\n",
    "# Histograms\n",
    "idxs = (0, 100000)  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "pred = model(data).detach().numpy()\n",
    "pred = np.multiply(pred, train_std.values)\n",
    "pred = np.add(pred, train_mean.values)\n",
    "data = np.multiply(data, train_std.values)\n",
    "data = np.add(data, train_mean.values)\n",
    "\n",
    "alph = 0.8\n",
    "n_bins = 50\n",
    "for kk in np.arange(4):\n",
    "    plt.figure(kk + 4)\n",
    "    n_hist_data, bin_edges, _ = plt.hist(data[:, kk], color=colors[1], label='Input', alpha=1, bins=n_bins)\n",
    "    n_hist_pred, _, _ = plt.hist(pred[:, kk], color=colors[0], label='Output', alpha=alph, bins=bin_edges)\n",
    "    plt.suptitle(train.columns[kk])\n",
    "    plt.xlabel(variable_list[kk] + ' ' + unit_list[kk])\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot input on top of output\n",
    "idxs = (0, 100)  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "pred = model(data).detach().numpy()\n",
    "pred = np.multiply(pred, train_std.values)\n",
    "pred = np.add(pred, train_mean.values)\n",
    "data = np.multiply(data, train_std.values)\n",
    "data = np.add(data, train_mean.values)\n",
    "\n",
    "for kk in np.arange(4):\n",
    "    plt.figure(kk)\n",
    "    plt.plot(data[:, kk], color=colors[1], label='Input', linestyle=line_style[1], marker=markers[1])\n",
    "    plt.plot(pred[:, kk], color=colors[0], label='Output', linestyle=line_style[0], marker=markers[0])\n",
    "    plt.suptitle(train.columns[kk])\n",
    "    plt.xlabel('Event')\n",
    "    plt.ylabel(variable_list[kk] + ' ' + unit_list[kk])\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input on top of output\n",
    "idxs = (0, int(1e5))  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "latent = model.encode(data).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in np.arange(latent.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.hist(latent[:, ii], label='$z_%d$' % (ii + 1), color='m')\n",
    "    plt.suptitle('Latent variable #%d' % (ii + 1))\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mksz = 1\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(latent[:, 0], latent[:, 1], s=mksz)\n",
    "plt.xlabel(r'$z_1$')\n",
    "plt.ylabel(r'$z_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (fastairoot)",
   "language": "python",
   "name": "fastairoot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
