{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Compression using an Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "BIN = '../../'\n",
    "sys.path.append(BIN)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from my_nn_modules import AE_big, get_data, fit\n",
    "\n",
    "import my_matplotlib_style as ms\n",
    "mpl.rc_file(BIN + 'my_matplotlib_rcparams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "force_cpu = False\n",
    "\n",
    "if force_cpu:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT</th>\n",
       "      <th>eta</th>\n",
       "      <th>phi</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1367643</td>\n",
       "      <td>300.752869</td>\n",
       "      <td>0.118391</td>\n",
       "      <td>-1.460827</td>\n",
       "      <td>303.719818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1739612</td>\n",
       "      <td>79.819145</td>\n",
       "      <td>4.535507</td>\n",
       "      <td>-1.185196</td>\n",
       "      <td>3722.832764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546066</td>\n",
       "      <td>220.472305</td>\n",
       "      <td>1.314261</td>\n",
       "      <td>-0.943992</td>\n",
       "      <td>440.195190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213212</td>\n",
       "      <td>74.533775</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>-0.424795</td>\n",
       "      <td>74.770332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150544</td>\n",
       "      <td>220.668121</td>\n",
       "      <td>2.432910</td>\n",
       "      <td>-2.308348</td>\n",
       "      <td>1266.681030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574837</td>\n",
       "      <td>159.841782</td>\n",
       "      <td>0.731125</td>\n",
       "      <td>-1.150598</td>\n",
       "      <td>207.133514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171687</td>\n",
       "      <td>306.125305</td>\n",
       "      <td>0.702473</td>\n",
       "      <td>0.437615</td>\n",
       "      <td>385.713013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608254</td>\n",
       "      <td>189.011673</td>\n",
       "      <td>-1.387443</td>\n",
       "      <td>0.490634</td>\n",
       "      <td>403.126709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1618830</td>\n",
       "      <td>194.996719</td>\n",
       "      <td>-1.022815</td>\n",
       "      <td>-1.560437</td>\n",
       "      <td>306.534515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637947</td>\n",
       "      <td>186.887146</td>\n",
       "      <td>-0.621282</td>\n",
       "      <td>-0.465523</td>\n",
       "      <td>226.002701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pT       eta       phi            E\n",
       "1367643  300.752869  0.118391 -1.460827   303.719818\n",
       "1739612   79.819145  4.535507 -1.185196  3722.832764\n",
       "546066   220.472305  1.314261 -0.943992   440.195190\n",
       "213212    74.533775  0.010658 -0.424795    74.770332\n",
       "150544   220.668121  2.432910 -2.308348  1266.681030\n",
       "574837   159.841782  0.731125 -1.150598   207.133514\n",
       "1171687  306.125305  0.702473  0.437615   385.713013\n",
       "1608254  189.011673 -1.387443  0.490634   403.126709\n",
       "1618830  194.996719 -1.022815 -1.560437   306.534515\n",
       "637947   186.887146 -0.621282 -0.465523   226.002701"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_pickle(BIN +'processed_data/train.pkl')\n",
    "test = pd.read_pickle(BIN + 'processed_data/test.pkl')\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and splitting into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "# Is this the right way to normalize? (only using train mean and std to normalize both train and test)\n",
    "test = (test - train_mean) / train_std\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are building and AE\n",
    "test_y = test_x\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target, size_average=None, reduce=None, reduction='mean'):\n",
    "    # type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -> Tensor\n",
    "    r\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
    "\n",
    "    Measures the element-wise mean squared error.\n",
    "\n",
    "    See :class:`~torch.nn.MSELoss` for details.\n",
    "    \"\"\"\n",
    "    if not (target.size() == input.size()):\n",
    "        warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n",
    "                      \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "                      \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
    "                      stacklevel=2)\n",
    "    if size_average is not None or reduce is not None:\n",
    "        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "\n",
    "    ret = (input - target) ** 2\n",
    "    factor = torch.full(ret.size(), 1)\n",
    "    factor[:, 2] = 2.4\n",
    "    ret = ret * factor\n",
    "    if reduction != 'none':\n",
    "        ret = torch.mean(ret) if reduction == 'mean' else torch.sum(ret)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256  # batch size\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "loss_func = nn.MSELoss()\n",
    "loss_func = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_big_2D_v1(nn.Module):\n",
    "    def __init__(self, n_features=4):\n",
    "        super(AE_big_2D_v1, self).__init__()\n",
    "        self.en1 = nn.Linear(n_features, 8)\n",
    "        self.en2 = nn.Linear(8, 6)\n",
    "        self.en3 = nn.Linear(6, 4)\n",
    "        self.en4 = nn.Linear(4, 2)\n",
    "        self.de1 = nn.Linear(2, 4)\n",
    "        self.de2 = nn.Linear(4, 6)\n",
    "        self.de3 = nn.Linear(6, 8)\n",
    "        self.de4 = nn.Linear(8, n_features)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "    \n",
    "\n",
    "class AE_big_2D_v2(nn.Module):\n",
    "    def __init__(self, n_features=4):\n",
    "        super(AE_big_2D_v2, self).__init__()\n",
    "        self.en1 = nn.Linear(n_features, 8)\n",
    "        self.en2 = nn.Linear(8, 6)\n",
    "        self.en3 = nn.Linear(6, 4)\n",
    "        self.en4 = nn.Linear(4, 3)\n",
    "        self.en5 = nn.Linear(3, 2)\n",
    "        self.de1 = nn.Linear(2, 3)\n",
    "        self.de2 = nn.Linear(3, 4)\n",
    "        self.de3 = nn.Linear(4, 6)\n",
    "        self.de4 = nn.Linear(6, 8)\n",
    "        self.de5 = nn.Linear(8, n_features)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.en5(self.tanh(self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.de5(self.tanh(self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(train.loc[0])\n",
    "model = AE_big_2D_v1(n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 1.0e-03\n",
      "Epoch 0: Validation loss = 0.38149009893100155 Time: 0:00:58.727954\n",
      "Epoch 1: Validation loss = 0.28856023618154747 Time: 0:01:57.806238\n",
      "Epoch 2: Validation loss = 0.21674779836272684 Time: 0:02:53.623118\n",
      "Epoch 3: Validation loss = 0.19915913820489253 Time: 0:03:49.725775\n",
      "Epoch 4: Validation loss = 0.19180766112009226 Time: 0:04:45.708774\n",
      "Epoch 5: Validation loss = 0.1843556405510303 Time: 0:05:41.229077\n",
      "Epoch 6: Validation loss = 0.18083966370212753 Time: 0:06:37.605944\n",
      "Epoch 7: Validation loss = 0.17489820434775802 Time: 0:07:33.178843\n",
      "Epoch 8: Validation loss = 0.1715156065029342 Time: 0:08:28.907761\n",
      "Epoch 9: Validation loss = 0.16301912259977233 Time: 0:09:25.877953\n",
      "Epoch 10: Validation loss = 0.1595075615339675 Time: 0:10:21.927872\n",
      "Epoch 11: Validation loss = 0.15538376848531715 Time: 0:11:17.509546\n",
      "Epoch 12: Validation loss = 0.15390258659924153 Time: 0:12:13.980747\n",
      "Epoch 13: Validation loss = 0.1505364980472544 Time: 0:13:09.747037\n",
      "Epoch 14: Validation loss = 0.1475411364661885 Time: 0:14:05.993115\n",
      "Epoch 15: Validation loss = 0.14429562217866024 Time: 0:15:02.972504\n",
      "Epoch 16: Validation loss = 0.141755645278387 Time: 0:16:00.140829\n",
      "Epoch 17: Validation loss = 0.1394526469861921 Time: 0:16:57.967300\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "#epochs_list = [7, 5, 3, 2, 2]\n",
    "epochs_list = [40, 40, 20, 20, 20]\n",
    "lrs = [1e-3, 3e-4, 1e-4, 3e-5, 1e-5]\n",
    "for ii, epochs in enumerate(epochs_list):\n",
    "    print('Setting learning rate to %.1e' % lrs[ii])\n",
    "    opt = optim.Adam(model.parameters(), lr=lrs[ii])\n",
    "    #opt = optim.SGD(model.parameters(), lr=lrs[ii], momentum=0.9, nesterov=True)\n",
    "    #opt = optim.RMSprop(model.parameters(), lr=lrs[ii], momentum=0.9)\n",
    "    fit(epochs, model, loss_func, opt, train_dl, valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model for later inference (if training is to be continued another saving method is recommended)\n",
    "#save_path = './models/AE_2D_v2_bs64_loss00991.pt'\n",
    "#torch.save(model.state_dict(), save_path)\n",
    "# model_big = AE_big()\n",
    "# model_big.load_state_dict(torch.load(save_path))\n",
    "# model_big.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few tensors, still normalized\n",
    "print('Comparing input and output:')\n",
    "for ii in np.arange(100, 105):\n",
    "    data = valid_ds.tensors[0][ii]\n",
    "    pred = model(data)\n",
    "    print('Inp:', data)\n",
    "    print('Out:', pred)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few tensors, now not normalized\n",
    "print('Comparing input and output:')\n",
    "for ii in np.arange(100, 105):\n",
    "    data = valid_ds.tensors[0][ii]\n",
    "    pred = model(data).detach().numpy()\n",
    "    pred = np.multiply(pred, train_std.values)\n",
    "    pred = np.add(pred, train_mean.values)\n",
    "    data = np.multiply(data, train_std.values)\n",
    "    data = np.add(data, train_mean.values)\n",
    "    print('Inp:', data)\n",
    "    print('Out:', pred)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']\n",
    "variable_list = [r'$p_T$', r'$\\eta$', r'$\\phi$', r'$E$']\n",
    "line_style = ['--', '-']\n",
    "colors = ['orange', 'c']\n",
    "markers = ['*', 's']\n",
    "\n",
    "\n",
    "# Histograms\n",
    "idxs = (0, 100000)  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "pred = model(data).detach().numpy()\n",
    "pred = np.multiply(pred, train_std.values)\n",
    "pred = np.add(pred, train_mean.values)\n",
    "data = np.multiply(data, train_std.values)\n",
    "data = np.add(data, train_mean.values)\n",
    "\n",
    "alph = 0.8\n",
    "n_bins = 50\n",
    "for kk in np.arange(4):\n",
    "    plt.figure(kk + 4)\n",
    "    n_hist_data, bin_edges, _ = plt.hist(data[:, kk], color=colors[1], label='Input', alpha=1, bins=n_bins)\n",
    "    n_hist_pred, _, _ = plt.hist(pred[:, kk], color=colors[0], label='Output', alpha=alph, bins=bin_edges)\n",
    "    plt.suptitle(train.columns[kk])\n",
    "    plt.xlabel(variable_list[kk] + ' ' + unit_list[kk])\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot input on top of output\n",
    "idxs = (0, 100)  # Choose events to compare\n",
    "data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "pred = model(data).detach().numpy()\n",
    "pred = np.multiply(pred, train_std.values)\n",
    "pred = np.add(pred, train_mean.values)\n",
    "data = np.multiply(data, train_std.values)\n",
    "data = np.add(data, train_mean.values)\n",
    "\n",
    "for kk in np.arange(4):\n",
    "    plt.figure(kk)\n",
    "    plt.plot(data[:, kk], color=colors[1], label='Input', linestyle=line_style[1], marker=markers[1])\n",
    "    plt.plot(pred[:, kk], color=colors[0], label='Output', linestyle=line_style[0], marker=markers[0])\n",
    "    plt.suptitle(train.columns[kk])\n",
    "    plt.xlabel('Event')\n",
    "    plt.ylabel(variable_list[kk] + ' ' + unit_list[kk])\n",
    "    plt.legend()\n",
    "    ms.sciy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (fastairoot)",
   "language": "python",
   "name": "fastairoot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
